{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "880a8910",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6fb328",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (921157865.py, line 6)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mBATCH_SIZE = 64``\u001b[39m\n                   ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = 'data/ebay/Tagged_Titles_Train.tsv'\n",
    "LIST_PATH = 'data/ebay/Listing_Titles.tsv'\n",
    "\n",
    "SAVE_DIRECTORY = \"./saved_models\"\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 64\n",
    "FINE_TUNE_BERT = False # set to true to train BERT as well\n",
    "LR = 2e-5 if FINE_TUNE_BERT else 1e-3\n",
    "MODEL_NAME = \"GottBERT/GottBERT_base_last\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fb1c3d",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb669b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModel, AutoConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import traceback \n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE=device\n",
    "print(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "tokenizer = tokenizer\n",
    "\n",
    "tagged_train = pd.read_csv(TRAIN_PATH, sep='\\t')\n",
    "os.makedirs(SAVE_DIRECTORY, exist_ok=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7dbfa5",
   "metadata": {},
   "source": [
    "## MLP Classifier Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b261560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dims=[64, 256, 128], num_classes=59, dropout=0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): Size of input features.\n",
    "            hidden_dims (list): List of hidden layer sizes. Pass [] for a single linear layer.\n",
    "            num_classes (int): Number of output classes.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        current_dim = input_dim\n",
    "\n",
    "        # Dynamically create hidden layers\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(current_dim, h_dim))\n",
    "            layers.append(nn.LayerNorm(h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            current_dim = h_dim\n",
    "\n",
    "        # Final output layer (Project to num_classes)\n",
    "        layers.append(nn.Linear(current_dim, num_classes))\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19880111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_original_classes(df):\n",
    "    tags = df['Tag'].fillna(\"\").astype(str).str.strip()\n",
    "    unique_tags = tags[tags != ''].unique()\n",
    "    sorted_tags = sorted(unique_tags)\n",
    "    print(f\"--- Original Class Analysis ---\")\n",
    "    print(f\"Total Unique Classes Found: {len(sorted_tags)}\")\n",
    "    print(f\"Classes: {sorted_tags}\")\n",
    "    \n",
    "    return sorted_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557e2a79",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf13af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "def preprocess_ebay_ner_data(df):\n",
    "    df['Tag'] = df['Tag'].fillna('')\n",
    "    all_raw_tags = df['Tag'].astype(str).str.strip().unique()\n",
    "    unique_aspects = set()\n",
    "    \n",
    "    for tag in all_raw_tags:\n",
    "        if tag not in ['', 'O', '0', 'nan']:\n",
    "            unique_aspects.add(tag)\n",
    "            \n",
    "    # 3. Construct the full label set (O + B-Tag + I-Tag for every aspect)\n",
    "    # This ensures 59 classes (1 'O' + 29*2) if all 29 aspects are in the data.\n",
    "    unique_labels = ['O']\n",
    "    for aspect in sorted(list(unique_aspects)):\n",
    "        unique_labels.append(f\"B-{aspect}\")\n",
    "        unique_labels.append(f\"I-{aspect}\")\n",
    "        \n",
    "    # Create mappings\n",
    "    # We sort again to ensure 'O' and the rest are in a deterministic order\n",
    "    unique_labels = sorted(unique_labels)\n",
    "    label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "    id2label = {i: label for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    print(f\"Generated {len(unique_labels)} classes from data.\")\n",
    "    grouped_titles = []\n",
    "    grouped_labels = []\n",
    "\n",
    "    for record_id, group in df.groupby('Record Number'):\n",
    "        tokens = group['Token'].astype(str).tolist()\n",
    "        raw_tags = group['Tag'].tolist()\n",
    "        bio_tags = []\n",
    "        current_entity_tag = None\n",
    "        for tag in raw_tags: \n",
    "            tag = tag.strip()\n",
    "            if tag == 'O' or tag == '0': \n",
    "                bio_tags.append('O')\n",
    "                current_entity_tag = None\n",
    "            elif tag != '':\n",
    "                bio_tags.append(f\"B-{tag}\")\n",
    "                current_entity_tag = tag\n",
    "            else:\n",
    "                if current_entity_tag:\n",
    "                    bio_tags.append(f\"I-{current_entity_tag}\")\n",
    "                else:\n",
    "                    bio_tags.append('O')\n",
    "\n",
    "        grouped_titles.append(tokens)\n",
    "        grouped_labels.append(bio_tags)\n",
    "    # unique_labels = sorted(list(set([l for seq in grouped_labels for l in seq])))\n",
    "    label2id = {label: i for i, label in enumerate(unique_labels)}\n",
    "    id2label = {i: label for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    return grouped_titles, grouped_labels, label2id, id2label \n",
    "\n",
    "\n",
    "class BertDataset(Dataset):\n",
    "    def __init__(self, token_lists, label_lists, label2id, max_len=128):\n",
    "        self.token_lists = token_lists\n",
    "        self.label_lists = label_lists\n",
    "        self.label2id = label2id\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.token_lists)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        word_list = self.token_lists[idx]\n",
    "        label_list = self.label_lists[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            word_list,\n",
    "            is_split_into_words=True,\n",
    "            padding='max_length',\n",
    "            truncation =True, \n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "        encoded_labels= []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                encoded_labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_str = label_list[word_idx]\n",
    "                encoded_labels.append(self.label2id[label_str])\n",
    "\n",
    "            else: \n",
    "                encoded_labels.append(-100)\n",
    "            \n",
    "            previous_word_idx = word_idx \n",
    "\n",
    "        labels_tensor = torch.tensor(encoded_labels, dtype=torch.long)\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = labels_tensor\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e05ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_and_prepare_data(train_csv_path, listing_csv_path=None):\n",
    "    \"\"\"\n",
    "    Loads the eBay NER datasets with specific parameters to handle \n",
    "    tab-separation and empty 'continuation' tags correctly.\n",
    "    \n",
    "    Args:\n",
    "        train_csv_path (str): Path to the tagged training data file.\n",
    "        listing_csv_path (str, optional): Path to the listing titles file (if needed for context).\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (train_df, token_lists, label_lists, label2id, id2label)\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {train_csv_path}...\")\n",
    "    \n",
    "    # 1. Load Training Data\n",
    "    # source: [113-116] - Specific pandas settings required:\n",
    "    # - sep=\"\\t\": Data is tab-separated\n",
    "    # - keep_default_na=False: Prevent pandas from turning empty strings into NaN\n",
    "    # - na_values=None: Ensure no other values are treated as NA\n",
    "    # - quoting: The file uses CSV-style quoting \n",
    "    try:\n",
    "        train_df = pd.read_csv(\n",
    "            train_csv_path, \n",
    "            sep=\"\\t\", \n",
    "            keep_default_na=False, \n",
    "            na_values=None,\n",
    "            quoting=csv.QUOTE_MINIMAL, # Handles standard CSV quoting used in the file\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading train data: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 2. Load Listing Data (Optional, context only)\n",
    "    # The training file already contains tokens, but this can be useful for EDA.\n",
    "    if listing_csv_path and os.path.exists(listing_csv_path):\n",
    "        listing_df = pd.read_csv(\n",
    "            listing_csv_path, \n",
    "            sep=\"\\t\", \n",
    "            keep_default_na=False, \n",
    "            na_values=None,\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        print(f\"Loaded {len(listing_df)} listing records.\")\n",
    "    \n",
    "    print(f\"Loaded {len(train_df)} training tokens.\")\n",
    "\n",
    "    # 3. Preprocess to BIO Format\n",
    "    # Re-using the logic defined in previous steps\n",
    "    print(\"Preprocessing data into BIO format...\")\n",
    "    labels = get_original_classes(train_df)\n",
    "    token_lists, label_lists, label2id, id2label = preprocess_ebay_ner_data(train_df)\n",
    "    \n",
    "    print(f\"Processed {len(token_lists)} sentences.\")\n",
    "    print(f\"Found {len(label2id)} unique classes (including 'O' and B-/I- prefixes).\")\n",
    "    \n",
    "    return train_df, token_lists, label_lists, label2id, id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c36512",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, grouped_titles, grouped_labels, label2id, id2label = load_and_prepare_data(TRAIN_PATH, LIST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e11736",
   "metadata": {},
   "source": [
    "## Dataset Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a41952",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = BertDataset(token_lists = grouped_titles, label_lists = grouped_labels, label2id = label2id, max_len=128)\n",
    "sample = total_dataset[1826]\n",
    "print(sample)\n",
    "print(tokenizer.convert_ids_to_tokens(sample['input_ids']))\n",
    "print(sample['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c74e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"MINI 1.6 W10B16A W11B16A R50 Steuerkettensatz 11311485400 Steuerkette FEBI 31803\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(example.split(\" \"), return_tensors=\"pt\", padding=True, is_split_into_words=True)\n",
    "tokens = tokens.to(DEVICE)\n",
    "print(tokens.word_ids())\n",
    "print(tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0]))\n",
    "with torch.no_grad():\n",
    "    outs = model(**tokens)\n",
    "\n",
    "print(outs.last_hidden_state.shape)\n",
    "classifier = MLPClassifier(input_dim=768, hidden_dims=[64], num_classes=59).to(DEVICE)\n",
    "preds = classifier(outs.last_hidden_state)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d868ee",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcf159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "def validate(bert_model, classifier, dataloader, device, id2label):\n",
    "    bert_model.eval()\n",
    "    classifier.eval()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_masks = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = bert_model(input_ids, attention_mask=attention_masks)\n",
    "            logits = classifier(outputs.last_hidden_state)\n",
    "            loss = loss_fn(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "            running_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=2)\n",
    "            mask = labels != -100 \n",
    "            all_preds.extend(preds[mask].cpu().numpy())\n",
    "            all_labels.extend(labels[mask].cpu().numpy())\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    unique_labels = sorted(list(set(all_labels)))\n",
    "    target_names = [id2label[i] for i in unique_labels]\n",
    "    report = classification_report(\n",
    "        all_labels,\n",
    "        all_preds, \n",
    "        labels=unique_labels,\n",
    "        target_names=target_names,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    return avg_loss, report\n",
    "\n",
    "def plot_results(train_loss_history, val_loss_history, final_report, epochs):\n",
    "    \"\"\"\n",
    "    Plots training vs validation loss and per-class F1 scores.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    # Plot 1: Loss Curves\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss_history, label='Train Loss', color='#1f77b4', linewidth=2)\n",
    "    \n",
    "    # We plot val loss as points or a line corresponding to epochs\n",
    "    # Since val_loss is recorded once per epoch, we stretch it or plot it against epoch indices\n",
    "    # Here we just plot the sequence of validation losses\n",
    "    plt.plot(np.linspace(0, len(train_loss_history), len(val_loss_history)), \n",
    "             val_loss_history, label='Val Loss', color='#d62728', marker='o', linewidth=2)\n",
    "    \n",
    "    plt.title(f\"Loss Curve over {epochs} Epochs\")\n",
    "    plt.xlabel(\"Training Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Top 20 Class F1 Scores\n",
    "    plt.subplot(1, 2, 2)\n",
    "    class_metrics = {k: v['f1-score'] for k, v in final_report.items() \n",
    "                     if isinstance(v, dict) and k not in ['accuracy', 'macro avg', 'weighted avg']}\n",
    "    \n",
    "    # Sort and slice top 20\n",
    "    sorted_classes = sorted(class_metrics.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    names = [x[0] for x in sorted_classes]\n",
    "    scores = [x[1] for x in sorted_classes]\n",
    "    \n",
    "    sns.barplot(x=scores, y=names, palette='viridis', hue=names, legend=False)\n",
    "    plt.title(f\"Top 20 F1 Scores (Epoch {epochs})\")\n",
    "    plt.xlabel(\"F1 Score\")\n",
    "    plt.xlim(0, 1.0)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def train_model(bert_model, classifier, train_loader, val_loader, label2id, id2label):\n",
    "    print(f\"Initializing Models... (Fine-tune Bert: {FINE_TUNE_BERT})\")\n",
    "    for param in bert_model.parameters():\n",
    "        param.requires_grad = FINE_TUNE_BERT\n",
    "    \n",
    "    params_to_optimize = list(classifier.parameters())\n",
    "    if FINE_TUNE_BERT:\n",
    "        params_to_optimize += list(bert_model.parameters())\n",
    "    optimizer = torch.optim.AdamW(params_to_optimize, lr=LR)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index = -100)\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    final_val_report = {}\n",
    "    try:\n",
    "        print(f\"Starting Training for {EPOCHS} Epochs... \")\n",
    "        for epoch in range(EPOCHS):\n",
    "            bert_model.train()\n",
    "            classifier.train()\n",
    "            running_train_loss = 0.0\n",
    "            progress_bar = tqdm(train_loader, desc=f\"EPOCH {epoch + 1}/ {EPOCHS}\")\n",
    "            for batch in progress_bar: \n",
    "                input_ids = batch['input_ids'].to(DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "                labels = batch['labels'].to(DEVICE)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward Pass\n",
    "                outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "                logits = classifier(outputs.last_hidden_state)\n",
    "                \n",
    "                # Loss\n",
    "                loss = loss_fn(logits.view(-1, len(label2id)), labels.view(-1))\n",
    "                \n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Track Loss\n",
    "                running_train_loss += loss.item()\n",
    "                train_loss_history.append(loss.item())\n",
    "                \n",
    "                # Update Progress Bar\n",
    "                progress_bar.set_postfix({'loss': loss.item()})\n",
    "            print(f\"Validating Epoch {epoch+1}... \")\n",
    "            val_loss, val_report = validate(bert_model, classifier, val_loader, DEVICE, id2label)\n",
    "            val_loss_history.append(val_loss)\n",
    "            final_val_report = val_report\n",
    "\n",
    "            print(f\"Epoch {epoch+1} Summary | Train Loss: {running_train_loss/len(train_loader):.4f} | Val Loss: {val_loss:.4f}\")\n",
    "            print(val_report)\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "    finally:\n",
    "        torch.save(bert_model.state_dict(), os.path.join(SAVE_DIRECTORY, \"bert_model.pth\"))\n",
    "        torch.save(classifier.state_dict(), os.path.join(SAVE_DIRECTORY, \"classifier_head.pth\"))\n",
    "        plot_results(train_loss_history, val_loss_history, final_val_report, EPOCHS)\n",
    "    \n",
    "    return bert_model, classifier, train_loss_history, val_loss_history, final_val_report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc40c23",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42fdd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_toks, val_toks, train_labs, val_labs = train_test_split(\n",
    "    grouped_titles, grouped_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "train_ds = BertDataset(train_toks, train_labs, label2id)\n",
    "val_ds = BertDataset(val_toks, val_labs, label2id)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "bert_model, classifier, train_loss, val_loss, report = train_model(\n",
    "    model, classifier, train_loader, val_loader, label2id, id2label\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
